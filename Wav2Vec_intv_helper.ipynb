{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9NAiUidhAll9HLF72LkXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmm-ai/affect_whisperer/blob/main/Wav2Vec_intv_helper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53zAtS2Xz8jw",
        "outputId": "cfb07a47-d7aa-494d-d4f2-429a4e2f0a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POoWtmvt4rWx",
        "outputId": "2b6fc278-ded7-4730-c669-674684309ad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.9/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.9/dist-packages (from resampy) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from resampy) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.53->resampy) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.53->resampy) (0.39.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# drive.flush_and_unmount()\n",
        "\n",
        "%cd /content/drive/MyDrive/Interview_Helper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu-ETi5R1b0q",
        "outputId": "5c89c7aa-b539-432c-8252-694b08b08b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Interview_Helper\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "mpxP1xBn0sGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "mJnLyuOq2GiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data_tess(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    label_map = {\n",
        "        \"angry\": 0,\n",
        "        \"disgust\": 1,\n",
        "        \"fear\": 2,\n",
        "        \"happy\": 3,\n",
        "        \"neutral\": 4,\n",
        "        \"sad\": 5,\n",
        "        \"surprise\": 6\n",
        "    }\n",
        "    for emotion in label_map:\n",
        "        file_list = glob.glob(os.path.join(directory, f\"{emotion}_*.wav\"))\n",
        "        for file in file_list:\n",
        "            audio, _ = librosa.load(file, sr=16000)\n",
        "            data.append(audio)\n",
        "            labels.append(label_map[emotion])\n",
        "    return data, labels\n",
        "\n",
        "def load_data_ravdess(directory):\n",
        "    data = []\n",
        "    labels = []\n",
        "    file_list = glob.glob(os.path.join(directory, \"Actor_*/*.wav\"))\n",
        "    for file in file_list:\n",
        "        emotion = int(os.path.basename(file).split(\"-\")[2]) - 1\n",
        "        audio, _ = librosa.load(file, sr=16000)\n",
        "        data.append(audio)\n",
        "        labels.append(emotion)\n",
        "    return data, labels\n",
        "\n",
        "def preprocess_data(data, labels, sr=16000, duration=3):\n",
        "    preprocessed_data = []\n",
        "    preprocessed_labels = []\n",
        "\n",
        "    for i, audio in enumerate(data):\n",
        "        length = len(audio)\n",
        "        target_length = sr * duration\n",
        "        if length >= target_length:\n",
        "            start = (length - target_length) // 2\n",
        "            end = start + target_length\n",
        "            preprocessed_data.append(audio[start:end])\n",
        "        else:\n",
        "            padding = (target_length - length) // 2\n",
        "            preprocessed_data.append(np.pad(audio, (padding, target_length - length - padding), mode='constant'))\n",
        "        preprocessed_labels.append(labels[i])\n",
        "\n",
        "    return preprocessed_data, preprocessed_labels\n",
        "\n",
        "# Load and preprocess TESS data\n",
        "tess_data, tess_labels = load_data_tess(\"/content/drive/MyDrive/Interview_Helper/RAVDESS_data/Audio_Speech_Actors_01-24\")\n",
        "tess_data, tess_labels = preprocess_data(tess_data, tess_labels)\n",
        "\n",
        "# Load and preprocess RAVDESS data\n",
        "ravdess_data, ravdess_labels = load_data_ravdess(\"RAVDESS\")\n",
        "ravdess_data, ravdess_labels = preprocess_data(ravdess_data, ravdess_labels)\n",
        "\n",
        "# Combine TESS and RAVDESS datasets\n",
        "combined_data = tess_data + ravdess_data\n",
        "combined_labels = tess_labels + ravdess_labels\n",
        "\n",
        "# Split data into train and eval sets\n",
        "train_data, eval_data, train_labels, eval_labels = train_test_split(combined_data, combined_labels, test_size=0.2, random_state=42, stratify=combined_labels)\n"
      ],
      "metadata": {
        "id": "m4yy-ZpESjyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(dataset_path, sample_rate=22050, duration=4):\n",
        "    features = []\n",
        "    labels = []\n",
        "    num_samples = sample_rate * duration\n",
        "\n",
        "    # Iterate through all the folders in the dataset\n",
        "    for actor_dir in os.listdir(dataset_path):\n",
        "        actor_path = os.path.join(dataset_path, actor_dir)\n",
        "        \n",
        "        # Iterate through all the audio files in each folder\n",
        "        for audio_file in os.listdir(actor_path):\n",
        "            file_path = os.path.join(actor_path, audio_file)\n",
        "            \n",
        "            # Load the audio file\n",
        "            audio, _ = librosa.load(file_path, sr=sample_rate, duration=duration, res_type='kaiser_fast')\n",
        "            \n",
        "            # Pad or truncate audio to the desired duration\n",
        "            audio = librosa.util.pad_center(audio, size=num_samples)\n",
        "            \n",
        "            # Get the emotion label from the file name\n",
        "            emotion = int(audio_file.split(\"-\")[2]) - 1  # Subtract 1 to make labels zero-indexed\n",
        "            \n",
        "            # Add features and labels to the lists\n",
        "            features.append(audio)\n",
        "            labels.append(emotion)\n",
        "    \n",
        "    features = np.array(features)\n",
        "    labels = np.array(labels)\n",
        "    \n",
        "    return features, labels\n"
      ],
      "metadata": {
        "id": "rhf_oRTQ0r79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features = np.load('/content/drive/MyDrive/Interview_Helper/features_Wav2Vec.npy')\n",
        "# labels = np.load('/content/drive/MyDrive/Interview_Helper/labels_Wav2Vec.npy')"
      ],
      "metadata": {
        "id": "hOn6Bquk1Una"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx], dtype=torch.float32).squeeze(0)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return feature, label\n"
      ],
      "metadata": {
        "id": "vZaiiXRJ3ccl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "features, labels = load_and_preprocess_data('/content/drive/MyDrive/Interview_Helper/RAVDESS_data/Audio_Speech_Actors_01-24')\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Dataset objects''\n",
        "train_dataset = EmotionDataset(X_train, y_train)\n",
        "val_dataset = EmotionDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoader objects\n",
        "batch_size = 32  #16,32,64,128,256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "cejLqZsU0rrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save features and labels to Google Drive\n",
        "np.save('/content/drive/MyDrive/Interview_Helper/features_Wav2Vec.npy', features)\n",
        "np.save('/content/drive/MyDrive/Interview_Helper/labels_Wav2Vec.npy', labels)"
      ],
      "metadata": {
        "id": "Hl47DDbk1M5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Wav2Vec2 model\n",
        "num_emotions = 8\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base\", num_labels=num_emotions)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz7DT0-Z06kg",
        "outputId": "0f972a33-3e84-412e-ee6f-67e0d540a684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForSequenceClassification: ['project_q.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.bias', 'project_hid.weight']\n",
            "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['projector.weight', 'classifier.weight', 'projector.bias', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "lr = 1e-5   #  1e-5, 3e-5, 1e-4, 1e-3, 5e-3, 1e-2, 5e-2\n",
        "num_epochs = 30  # 10, 20 30, 50, 100, 200\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "patience = 4\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for batch in train_loader:\n",
        "        features, labels = batch\n",
        "        features = features.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(features).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataset)\n",
        "    train_acc = total_correct / len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            features, labels = batch\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(features).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "\n",
        "    val_loss = total_loss / len(val_dataset)\n",
        "    val_acc = total_correct / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement == patience:\n",
        "            print(\"Early stopping due to no improvement in validation loss for {} epochs.\".format(patience))\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcu5HWeY1Az3",
        "outputId": "3253d9e7-5db9-46d4-e7eb-fcef8c9eb964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "Train Loss: 0.0643, Train Acc: 0.2535\n",
            "Val Loss: 0.0630, Val Acc: 0.2917\n",
            "Epoch 2/30\n",
            "Train Loss: 0.0606, Train Acc: 0.3446\n",
            "Val Loss: 0.0576, Val Acc: 0.3333\n",
            "Epoch 3/30\n",
            "Train Loss: 0.0545, Train Acc: 0.4627\n",
            "Val Loss: 0.0509, Val Acc: 0.5486\n",
            "Epoch 4/30\n",
            "Train Loss: 0.0480, Train Acc: 0.5773\n",
            "Val Loss: 0.0449, Val Acc: 0.5833\n",
            "Epoch 5/30\n",
            "Train Loss: 0.0416, Train Acc: 0.6519\n",
            "Val Loss: 0.0402, Val Acc: 0.6215\n",
            "Epoch 6/30\n",
            "Train Loss: 0.0348, Train Acc: 0.7665\n",
            "Val Loss: 0.0359, Val Acc: 0.6562\n",
            "Epoch 7/30\n",
            "Train Loss: 0.0301, Train Acc: 0.8021\n",
            "Val Loss: 0.0320, Val Acc: 0.7222\n",
            "Epoch 8/30\n",
            "Train Loss: 0.0252, Train Acc: 0.8585\n",
            "Val Loss: 0.0302, Val Acc: 0.7153\n",
            "Epoch 9/30\n",
            "Train Loss: 0.0211, Train Acc: 0.8889\n",
            "Val Loss: 0.0276, Val Acc: 0.7361\n",
            "Epoch 10/30\n",
            "Train Loss: 0.0184, Train Acc: 0.9036\n",
            "Val Loss: 0.0246, Val Acc: 0.7882\n",
            "Epoch 11/30\n",
            "Train Loss: 0.0160, Train Acc: 0.9201\n",
            "Val Loss: 0.0240, Val Acc: 0.7917\n",
            "Epoch 12/30\n",
            "Train Loss: 0.0140, Train Acc: 0.9288\n",
            "Val Loss: 0.0276, Val Acc: 0.7361\n",
            "Epoch 13/30\n",
            "Train Loss: 0.0123, Train Acc: 0.9392\n",
            "Val Loss: 0.0219, Val Acc: 0.7917\n",
            "Epoch 14/30\n",
            "Train Loss: 0.0092, Train Acc: 0.9740\n",
            "Val Loss: 0.0211, Val Acc: 0.8021\n",
            "Epoch 15/30\n",
            "Train Loss: 0.0080, Train Acc: 0.9766\n",
            "Val Loss: 0.0223, Val Acc: 0.7917\n",
            "Epoch 16/30\n",
            "Train Loss: 0.0085, Train Acc: 0.9618\n",
            "Val Loss: 0.0216, Val Acc: 0.8021\n",
            "Epoch 17/30\n",
            "Train Loss: 0.0067, Train Acc: 0.9766\n",
            "Val Loss: 0.0208, Val Acc: 0.7951\n",
            "Epoch 18/30\n",
            "Train Loss: 0.0053, Train Acc: 0.9948\n",
            "Val Loss: 0.0190, Val Acc: 0.8229\n",
            "Epoch 19/30\n",
            "Train Loss: 0.0053, Train Acc: 0.9826\n",
            "Val Loss: 0.0221, Val Acc: 0.8021\n",
            "Epoch 20/30\n",
            "Train Loss: 0.0071, Train Acc: 0.9566\n",
            "Val Loss: 0.0324, Val Acc: 0.7014\n",
            "Epoch 21/30\n",
            "Train Loss: 0.0053, Train Acc: 0.9757\n",
            "Val Loss: 0.0225, Val Acc: 0.7882\n",
            "Epoch 22/30\n",
            "Train Loss: 0.0041, Train Acc: 0.9878\n",
            "Val Loss: 0.0189, Val Acc: 0.8229\n",
            "Epoch 23/30\n",
            "Train Loss: 0.0038, Train Acc: 0.9905\n",
            "Val Loss: 0.0203, Val Acc: 0.8264\n",
            "Epoch 24/30\n",
            "Train Loss: 0.0039, Train Acc: 0.9809\n",
            "Val Loss: 0.0238, Val Acc: 0.7812\n",
            "Epoch 25/30\n",
            "Train Loss: 0.0042, Train Acc: 0.9783\n",
            "Val Loss: 0.0246, Val Acc: 0.7569\n",
            "Epoch 26/30\n",
            "Train Loss: 0.0036, Train Acc: 0.9852\n",
            "Val Loss: 0.0192, Val Acc: 0.8299\n",
            "Epoch 27/30\n",
            "Train Loss: 0.0033, Train Acc: 0.9809\n",
            "Val Loss: 0.0189, Val Acc: 0.8264\n",
            "Epoch 28/30\n",
            "Train Loss: 0.0036, Train Acc: 0.9818\n",
            "Val Loss: 0.0189, Val Acc: 0.8438\n",
            "Epoch 29/30\n",
            "Train Loss: 0.0044, Train Acc: 0.9679\n",
            "Val Loss: 0.0228, Val Acc: 0.8125\n",
            "Epoch 30/30\n",
            "Train Loss: 0.0030, Train Acc: 0.9852\n",
            "Val Loss: 0.0211, Val Acc: 0.8160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class PyTorchModelWrapper(BaseEstimator):\n",
        "    def __init__(self, model_class, criterion, device, batch_size=32, learning_rate=1e-3, num_epochs=10):\n",
        "        self.model_class = model_class\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Create Datasets and DataLoaders\n",
        "        train_dataset = EmotionDataset(X, y)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        # Initialize the model, optimizer, and move them to the device\n",
        "        model = self.model_class().to(self.device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.num_epochs):\n",
        "            train_model(model, train_loader, self.criterion, optimizer, self.device)\n",
        "\n",
        "        self.model_ = model\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        val_dataset = EmotionDataset(X, np.zeros(X.shape[0]))  # Dummy labels\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        self.model_.eval()\n",
        "\n",
        "        # Run the model on validation data\n",
        "        all_outputs = []\n",
        "        for features, _ in val_loader:\n",
        "            features = features.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model_(features)\n",
        "            all_outputs.append(outputs.cpu().numpy())\n",
        "\n",
        "        # Concatenate all outputs\n",
        "        return np.concatenate(all_outputs, axis=0)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return np.mean(y == np.argmax(y_pred, axis=1))\n"
      ],
      "metadata": {
        "id": "SSfaGW02DJAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory where you want to save the model\n",
        "output_dir = \"/content/drive/MyDrive/Interview_Helper/wav2vec2_emotion_model_Ravdess1e5_30epoch_8160\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "MObHhurY1nVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification\n",
        "\n",
        "# Load the saved model\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "-azHuMWk1oF2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}